ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
beta_spread <- ap_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
beta_spread <- energy_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
ggplot(beta_spread, aes(term, log_ratio)) +
geom_point() +
theme_bw()
ggplot(beta_spread, aes(log_ratio, term)) +
geom_point() +
theme_bw()
beta_spread <- energy_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .01 | topic2 > .01) %>%
mutate(log_ratio = log2(topic2 / topic1))
ggplot(beta_spread, aes(log_ratio, term)) +
geom_point() +
theme_bw()
energy_topics <- tidy(energy_lds, matrix = "beta")
ap_top_terms <- energy_topics %>%
group_by(topic) %>%
top_n(0, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
energy_lds <- LDA(text, k = 4, control = list(seed = 1234))
energy_topics <- tidy(energy_lds, matrix = "beta")
ap_top_terms <- energy_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
energy_lds <- LDA(text, k = 3, control = list(seed = 1234))
energy_lds
energy_topics <- tidy(energy_lds, matrix = "beta")
ap_top_terms <- energy_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
water_articles <- list.files(path = "../articles/water/", pattern = ".pdf")
water_articles <- paste0("../articles/water/", water_articles)
corp <- Corpus(URISource(water_articles, encoding = "UTF-8"),
readerControl = list(reader = readPDF))
text <- DocumentTermMatrix(corp,
control =
list(removePunctuation = TRUE,
stopwords = TRUE,
tolower = TRUE,
stemming = FALSE,
removeNumbers = TRUE,
bounds = list(global = c(1, Inf))))
water_lda <- LDA(text, k = 3, control = list(seed = 1234))
water_topics <- tidy(water_lda, matrix = "beta")
water_lda <- LDA(text, k = 3, control = list(seed = 1234))
water_topics <- tidy(water_lda, matrix = "beta")
ap_top_terms <- water_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
water_lda <- LDA(text, k = 2, control = list(seed = 1234))
water_topics <- tidy(water_lda, matrix = "beta")
ap_top_terms <- water_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
water_lda <- LDA(text, k = 4, control = list(seed = 1234))
water_topics <- tidy(water_lda, matrix = "beta")
ap_top_terms <- water_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
corp <- Corpus(URISource(energy_articles, encoding = "UTF-8"),
readerControl = list(reader = readPDF))
text <- DocumentTermMatrix(corp,
control =
list(removePunctuation = TRUE,
stopwords = TRUE,
tolower = TRUE,
stemming = TRUE,
removeNumbers = TRUE,
bounds = list(global = c(1, Inf))))
energy_lds <- LDA(text, k = 3, control = list(seed = 1234))
energy_lds
energy_topics <- tidy(energy_lds, matrix = "beta")
ap_top_terms <- energy_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
names(energy_topics)
energy_lds <- LDA(text, k = 2, control = list(seed = 1234))
energy_lds
energy_topics <- tidy(energy_lds, matrix = "beta")
ap_top_terms <- energy_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
energy_articles
corp <- Corpus(URISource(energy_articles[c(9, 6, 10:22)], encoding = "UTF-8"),
readerControl = list(reader = readPDF))
text <- DocumentTermMatrix(corp,
control =
list(removePunctuation = TRUE,
stopwords = TRUE,
tolower = TRUE,
stemming = TRUE,
removeNumbers = TRUE,
bounds = list(global = c(1, Inf))))
energy_lds <- LDA(text, k = 2, control = list(seed = 1234))
energy_lds
energy_topics <- tidy(energy_lds, matrix = "beta")
ap_top_terms <- energy_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
energy_lds <- LDA(text, k = 3, control = list(seed = 1234))
energy_lds
energy_topics <- tidy(energy_lds, matrix = "beta")
ap_top_terms <- energy_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
energy_lds <- LDA(text, k = 2, control = list(seed = 1234))
energy_lds
energy_topics <- tidy(energy_lds, matrix = "beta")
ap_top_terms <- energy_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
ap_top_terms <- energy_topics %>%
group_by(term) %>%
mutate(number = count(.))
ap_top_terms <- energy_topics %>%
group_by(term) %>%
count()
ap_top_terms
ap_top_terms <- energy_topics %>%
group_by(topic) %>%
top_n(25, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
energy_lds <- LDA(text, k = 4, control = list(seed = 1234))
energy_lds
energy_topics <- tidy(energy_lds, matrix = "beta")
ap_top_terms <- energy_topics %>%
group_by(topic) %>%
top_n(25, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
# Script for word frequency enumeration
library(tidyverse)
library(bibliometrix)
library(tm)
library(wordcloud2)
library(viridis)
library(maps)
# Import each text file
savedrecs1_orig <- readLines(con <- file("../Data/Inputs/savedrecs_infews_20200731_1to500.txt"))
# Import each text file
savedrecs1_orig <- readLines(con <- file("../data/savedrecs_infews_20200731_1to500.txt"))
savedrecs2_orig <- readLines(con <- file("../data/savedrecs_infews_20200731_501to892.txt"))
# Remove the first two rows of each original file
savedrecs1 <- as.character(data.frame(savedrecs1_orig)[-c(1:2),])
savedrecs2 <- as.character(data.frame(savedrecs2_orig)[-c(1:2),])
savedrecs <- c(savedrecs1, savedrecs2)
df_wide <- convert2df(file = savedrecs,
dbsource = "isi", format = "plaintext")
# First create text_in with only the abstracts
text_in <- df_wide$AB
# Remove NAs
text_in <- text_in[which(is.na(text_in)==FALSE)]
# For these procedures, object needs to be of class Corpus
text0 <- Corpus(VectorSource(text_in))
# Various text processing steps.
text <- TermDocumentMatrix(text0,
control =
list(removePunctuation = TRUE,
stopwords = TRUE,
tolower = TRUE,
stemming = FALSE,
removeNumbers = TRUE,
bounds = list(global = c(1, Inf))))
ft <- findFreqTerms(text, lowfreq = 20, highfreq = Inf)
ft_matrix <- as.matrix(text[ft,])
sorted_matrix <- sort(apply(ft_matrix, 1, sum), decreasing = TRUE)
sorted_matrix
write.csv(sorted_matrix, "../data/webofscience_wf_output_table.csv")
df_wide <- convert2df(file = savedrecs,
dbsource = "isi", format = "plaintext") %>%
mutate(abstract_food_contains = ifelse(grepl(pattern = "food", AB, ignore.case = TRUE) &
grepl(pattern = "water", AB, ignore.case = TRUE) &
grepl(pattern = "energy", AB, ignore.case = TRUE), 1, 0))
summary(df_wide$abstract_food_contains)
df_wide <- convert2df(file = savedrecs,
dbsource = "isi", format = "plaintext") %>%
mutate(abstract_infews_contains = ifelse(grepl(pattern = "food", AB, ignore.case = TRUE) &
grepl(pattern = "water", AB, ignore.case = TRUE) &
grepl(pattern = "energy", AB, ignore.case = TRUE), 1, 0),
title_infews_contains = ifelse(grepl(pattern = "food", TI, ignore.case = TRUE) &
grepl(pattern = "water", TI, ignore.case = TRUE) &
grepl(pattern = "energy", TI, ignore.case = TRUE), 1, 0),
keywords_infews_contains = ifelse(grepl(pattern = "food", DE, ignore.case = TRUE) &
grepl(pattern = "water", DE, ignore.case = TRUE) &
grepl(pattern = "energy", DE, ignore.case = TRUE), 1, 0),
wos_key_infews_contains = ifelse(grepl(pattern = "food", ID, ignore.case = TRUE) &
grepl(pattern = "water", ID, ignore.case = TRUE) &
grepl(pattern = "energy", ID, ignore.case = TRUE), 1, 0)) %>%
filter(!(abstract_infews_contains == 0 & title_infews_contains == 0 &
keywords_infews_contains == 0 & wos_key_infews_contains == 1))
# First create text_in with only the abstracts
text_in <- df_wide$AB
# Remove NAs
text_in <- text_in[which(is.na(text_in)==FALSE)]
# For these procedures, object needs to be of class Corpus
text0 <- Corpus(VectorSource(text_in))
# Various text processing steps.
text <- TermDocumentMatrix(text0,
control =
list(removePunctuation = TRUE,
stopwords = TRUE,
tolower = TRUE,
stemming = FALSE,
removeNumbers = TRUE,
bounds = list(global = c(1, Inf))))
ft <- findFreqTerms(text, lowfreq = 20, highfreq = Inf)
ft_matrix <- as.matrix(text[ft,])
sorted_matrix <- sort(apply(ft_matrix, 1, sum), decreasing = TRUE)
write.csv(sorted_matrix, "../data/webofscience_wf_output_table.csv")
# Various text processing steps.
text <- TermDocumentMatrix(text0,
control =
list(removePunctuation = TRUE,
stopwords = TRUE,
tolower = TRUE,
stemming = TRUE,
removeNumbers = TRUE,
bounds = list(global = c(1, Inf))))
ft <- findFreqTerms(text, lowfreq = 20, highfreq = Inf)
ft_matrix <- as.matrix(text[ft,])
sorted_matrix <- sort(apply(ft_matrix, 1, sum), decreasing = TRUE)
write.csv(sorted_matrix, "../data/webofscience_wf_output_table.csv")
# Various text processing steps.
text <- TermDocumentMatrix(text0,
control =
list(removePunctuation = TRUE,
stopwords = TRUE,
tolower = TRUE,
stemming = FALSE,
removeNumbers = TRUE,
bounds = list(global = c(1, Inf))))
ft <- findFreqTerms(text, lowfreq = 20, highfreq = Inf)
ft_matrix <- as.matrix(text[ft,])
sorted_matrix <- sort(apply(ft_matrix, 1, sum), decreasing = TRUE)
write.csv(sorted_matrix, "../data/webofscience_wf_output_table.csv")
df_wide_subsample <- df_wide %>%
select(PY, DI, TI) %>%
sample_n(100) %>%
rownames_to_column(var = "row_number_flag") %>%
mutate(pertinent = NA,
row_number_flag = as.numeric(row_number_flag),
reviewer = ifelse(between(row_number_flag, 1, 50) , "Julie", NA),
reviewer = ifelse(between(row_number_flag, 51, 100) , "Michael", reviewer)) %>%
select(-row_number_flag)
df_wide_subsample <- df_wide %>%
select(PY, DI, TI) %>%
sample_n(100, weight = as.factor(PY)) %>%
rownames_to_column(var = "row_number_flag") %>%
mutate(pertinent = NA,
row_number_flag = as.numeric(row_number_flag),
reviewer = ifelse(between(row_number_flag, 1, 50) , "Julie", NA),
reviewer = ifelse(between(row_number_flag, 51, 100) , "Michael", reviewer)) %>%
select(-row_number_flag)
df_wide_subsample <- df_wide %>%
select(PY, DI, TI) %>%
sample_n(100) %>%
rownames_to_column(var = "row_number_flag") %>%
mutate(pertinent = NA,
row_number_flag = as.numeric(row_number_flag),
reviewer = ifelse(between(row_number_flag, 1, 50) , "Julie", NA),
reviewer = ifelse(between(row_number_flag, 51, 100) , "Michael", reviewer)) %>%
select(-row_number_flag)
write.csv(x = df_wide_subsample,
file = "../data/df_wide_subsample.csv",
row.names = FALSE)
CS <- conceptualStructure(df_wide, field="AB", method="CA",
stemming=FALSE, minDegree=35, k.max = 5)
df_wide_subsample <- df_wide %>%
select(PY, DI, TI, AB) %>%
sample_n(100) %>%
rownames_to_column(var = "row_number_flag") %>%
mutate(pertinent = NA,
row_number_flag = as.numeric(row_number_flag),
reviewer = ifelse(between(row_number_flag, 1, 50) , "Julie", NA),
reviewer = ifelse(between(row_number_flag, 51, 100) , "Michael", reviewer)) %>%
select(-row_number_flag)
write.csv(x = df_wide_subsample,
file = "../data/df_wide_subsample.csv",
row.names = FALSE)
CS <- conceptualStructure(df_wide, field="AB", method="CA",
stemming=FALSE, minDegree=70, k.max = 5)
CS <- conceptualStructure(df_wide, field="AB", method="CA",
stemming=FALSE, minDegree=100, k.max = 5)
CS <- conceptualStructure(df_wide, field="AB", method="CA",
stemming=FALSE, minDegree=250, k.max = 5)
summary(df_wide$PY)
summary(as.factor(df_wide$PY))
setwd("C:/Users/michael.f.meyer/Dropbox/Systematic_Reviews/esa_inspire_talk")
library(reshape2)
library(ggplot2)
library(tidyr)
library(dplyr)
library(stringr)
library(maps)
library(ggmap)
library(maptools)
library(rworldmap)
savedrecs1_orig <- readLines(con <- file("savedrecs_ppcps20200717_1to500.txt"))
savedrecs2_orig <- readLines(con <- file("savedrecs_ppcps20200717_501to1000.txt"))
savedrecs3_orig <- readLines(con <- file("savedrecs_ppcps20200717_1001to1500.txt"))
savedrecs4_orig <- readLines(con <- file("savedrecs_ppcps20200717_1501to2000.txt"))
savedrecs5_orig <- readLines(con <- file("savedrecs_ppcps20200717_2001to2500.txt"))
savedrecs6_orig <- readLines(con <- file("savedrecs_ppcps20200717_2501to3000.txt"))
savedrecs7_orig <- readLines(con <- file("savedrecs_ppcps20200717_3001to3500.txt"))
savedrecs8_orig <- readLines(con <- file("savedrecs_ppcps20200717_3501to4000.txt"))
savedrecs9_orig <- readLines(con <- file("savedrecs_ppcps20200717_4001to4500.txt"))
savedrecs10_orig <- readLines(con <- file("savedrecs_ppcps20200717_4501to5000.txt"))
savedrecs11_orig <- readLines(con <- file("savedrecs_ppcps20200717_5001to5500.txt"))
savedrecs12_orig <- readLines(con <- file("savedrecs_ppcps20200717_5501to6000.txt"))
savedrecs13_orig <- readLines(con <- file("savedrecs_ppcps20200717_6001to6500.txt"))
savedrecs14_orig <- readLines(con <- file("savedrecs_ppcps20200717_6501to7000.txt"))
savedrecs15_orig <- readLines(con <- file("savedrecs_ppcps20200717_7001to7383.txt"))
# Remove the first two rows of each original file
savedrecs1 <- as.character(data.frame(savedrecs1_orig)[-c(1:2),])
savedrecs2 <- as.character(data.frame(savedrecs2_orig)[-c(1:2),])
savedrecs3 <- as.character(data.frame(savedrecs3_orig)[-c(1:2),])
savedrecs4 <- as.character(data.frame(savedrecs4_orig)[-c(1:2),])
savedrecs5 <- as.character(data.frame(savedrecs5_orig)[-c(1:2),])
savedrecs6 <- as.character(data.frame(savedrecs6_orig)[-c(1:2),])
savedrecs7 <- as.character(data.frame(savedrecs7_orig)[-c(1:2),])
savedrecs8 <- as.character(data.frame(savedrecs8_orig)[-c(1:2),])
savedrecs9 <- as.character(data.frame(savedrecs9_orig)[-c(1:2),])
savedrecs10 <- as.character(data.frame(savedrecs10_orig)[-c(1:2),])
savedrecs11 <- as.character(data.frame(savedrecs11_orig)[-c(1:2),])
savedrecs12 <- as.character(data.frame(savedrecs12_orig)[-c(1:2),])
savedrecs13 <- as.character(data.frame(savedrecs13_orig)[-c(1:2),])
savedrecs14 <- as.character(data.frame(savedrecs14_orig)[-c(1:2),])
savedrecs15 <- as.character(data.frame(savedrecs15_orig)[-c(1:2),])
# Combine savedrecs files into one object
savedrecs <- c(savedrecs1,savedrecs2,savedrecs3,savedrecs4,
savedrecs5,savedrecs6,savedrecs7,savedrecs8,
savedrecs9, savedrecs10, savedrecs11, savedrecs12,
savedrecs13, savedrecs14, savedrecs15)
which <- grep("*PT *|*AU *|*AB *|*TI *|*SO *|*DI *|*PY *|*SC *|*WC *|*C1 *", savedrecs)
# Filter savedrecs by the indexed WOS fields in 'which'
savedrecs <- savedrecs[which]
# Make sure savedrecs is all characters
savedrecs <- as.character(savedrecs)
# Isolate the savedrecs WOS field
bibtype <- as.character(substr(savedrecs,1,2))
# Isolate the descriptions for each of WOS tags
desc <- substr(savedrecs,4,nchar(savedrecs))
# Build an index for each item
index <- rep(0,length(savedrecs))
# Create a dataframe of index, WOS tag, and description
df <- data.frame(index,bibtype,desc)
# Find the index for differing publications
new <- which(df$bibtype=="PT")
newindex <- 0
# Now, we build a new index where all indices are for a given pub
for(i in 1:length(df[,1])){
if(i %in% new){newindex <- newindex+1}
df$index[i] <- newindex
}
# Change bibtype to character, remove rows where bibtype is blank
df$bibtype <- as.character(df$bibtype)
df <- df[which(df$bibtype!="  "),]
# Pivot the long table format to wide format
df_wide <- dcast(df,index~bibtype)
# Script for word frequency enumeration
library(stringr)
library(tm)
# First create text_in with only the abstracts
text_in <- df_wide$AB
# Remove NAs
text_in <- text_in[which(is.na(text_in)==FALSE)]
# For these procedures, object needs to be of class Corpus
text0 <- Corpus(x = VectorSource(x = text_in))
# Various text processing steps.
text <-  TermDocumentMatrix(text0,
control =
list(removePunctuation = TRUE,
stopwords = TRUE,
tolower = TRUE,
stemming = FALSE,
removeNumbers = TRUE,
bounds = list(global = c(1, Inf))))
# Find the frequency of certain terms that appear at least
# 20 times, and then sort them in decreasing order.
# The final outputted matrix will be saved as a CSV.
ft <- findFreqTerms(text, lowfreq = 15, highfreq = Inf)
ft_matrix <- as.matrix(text[ft,])
sorted_matrix <- sort(apply(ft_matrix, 1, sum), decreasing = TRUE)
write.csv(sorted_matrix, "wf_output_table_20200717.csv")
